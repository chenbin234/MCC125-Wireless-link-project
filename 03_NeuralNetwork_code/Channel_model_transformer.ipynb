{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCC125 - Wireless Link Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import model_Transformer_modified\n",
    "import ast  # Used to parse complex numbers from string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file1 = 'simulation_feature_dataset.csv'\n",
    "output_file_feature = 'simulation_feature_dataset_processing.csv'\n",
    "\n",
    "input_file2 = 'simulation_target_dataset.csv'\n",
    "output_file_target = 'simulation_target_dataset_processing.csv'\n",
    "\n",
    "with open(input_file1, 'r', newline='') as infile, open(output_file_feature, 'w', newline='') as outfile:\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "\n",
    "    for row in reader:\n",
    "        new_row = [cell.replace('i', 'j') for cell in row]\n",
    "        writer.writerow(new_row)\n",
    "\n",
    "with open(input_file2, 'r', newline='') as infile, open(output_file_target, 'w', newline='') as outfile:\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "\n",
    "    for row in reader:\n",
    "        new_row = [cell.replace('i', 'j') for cell in row]\n",
    "        writer.writerow(new_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to parse complex numbers from string\n",
    "def complex_parser(s):\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = pd.read_csv(output_file_target, header=None, converters={col: complex_parser for col in range(len(pd.read_csv(output_file_target).columns))})\n",
    "\n",
    "df_feature = pd.read_csv(output_file_feature, header=None, converters={col: complex_parser for col in range(len(pd.read_csv(output_file_feature).columns))})\n",
    "\n",
    "# filter zero values rows in the dataframe\n",
    "# df_target = df_target[df_target.loc[:,0] != 0j]\n",
    "# df_feature = df_feature[df_feature.loc[:,0] != 0j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_of_rows = df_feature.index[df_feature.iloc[:, 0] == 0].tolist()\n",
    "len(index_of_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.574380+0.114876j</td>\n",
       "      <td>0.574380+0.038292j</td>\n",
       "      <td>0.191460+0.038292j</td>\n",
       "      <td>-0.421212+0.497796j</td>\n",
       "      <td>1.110467-0.344628j</td>\n",
       "      <td>-0.344628-1.110467j</td>\n",
       "      <td>-0.804132-0.727548j</td>\n",
       "      <td>-0.804132-0.038292j</td>\n",
       "      <td>0.344628-0.038292j</td>\n",
       "      <td>0.344628-0.114876j</td>\n",
       "      <td>-0.038292+0.497796j</td>\n",
       "      <td>0.574380+0.957299j</td>\n",
       "      <td>-1.187051+0.727548j</td>\n",
       "      <td>0.114876-0.344628j</td>\n",
       "      <td>-0.038292+0.957299j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.880716+0.114876j</td>\n",
       "      <td>0.880716+1.187051j</td>\n",
       "      <td>-0.650964+0.421212j</td>\n",
       "      <td>-0.268044+0.191460j</td>\n",
       "      <td>0.344628-0.268044j</td>\n",
       "      <td>-0.114876+0.650964j</td>\n",
       "      <td>-1.110467-0.344628j</td>\n",
       "      <td>1.033883+0.421212j</td>\n",
       "      <td>-0.497796-0.957299j</td>\n",
       "      <td>0.650964+0.880716j</td>\n",
       "      <td>-0.114876-0.114876j</td>\n",
       "      <td>0.574380-0.268044j</td>\n",
       "      <td>0.650964+0.880716j</td>\n",
       "      <td>-0.727548+0.268044j</td>\n",
       "      <td>0.957299+0.497796j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.421212-0.114876j</td>\n",
       "      <td>-0.114876-0.880716j</td>\n",
       "      <td>-1.033883+0.957299j</td>\n",
       "      <td>0.804132+0.497796j</td>\n",
       "      <td>-0.114876+1.033883j</td>\n",
       "      <td>0.957299+0.191460j</td>\n",
       "      <td>0.191460-1.110467j</td>\n",
       "      <td>0.191460-0.957299j</td>\n",
       "      <td>0.344628+0.727548j</td>\n",
       "      <td>0.574380-0.650964j</td>\n",
       "      <td>0.497796+0.574380j</td>\n",
       "      <td>-0.650964+0.421212j</td>\n",
       "      <td>-1.033883+0.574380j</td>\n",
       "      <td>-0.268044+0.114876j</td>\n",
       "      <td>-0.191460-1.110467j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.574380+0.650964j</td>\n",
       "      <td>0.574380-0.804132j</td>\n",
       "      <td>-0.804132-0.344628j</td>\n",
       "      <td>-0.957299-0.957299j</td>\n",
       "      <td>0.421212+0.804132j</td>\n",
       "      <td>-0.114876+0.650964j</td>\n",
       "      <td>0.038292+0.268044j</td>\n",
       "      <td>0.114876+0.268044j</td>\n",
       "      <td>0.497796-0.574380j</td>\n",
       "      <td>-0.727548+1.110467j</td>\n",
       "      <td>-0.650964+0.268044j</td>\n",
       "      <td>-0.421212+0.344628j</td>\n",
       "      <td>-1.110467+0.804132j</td>\n",
       "      <td>-0.114876+1.187051j</td>\n",
       "      <td>-0.880716+1.187051j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.880716-0.344628j</td>\n",
       "      <td>1.033883+0.804132j</td>\n",
       "      <td>-0.650964-0.421212j</td>\n",
       "      <td>0.268044-0.727548j</td>\n",
       "      <td>-1.110467+0.114876j</td>\n",
       "      <td>-1.187051-1.110467j</td>\n",
       "      <td>0.574380+0.421212j</td>\n",
       "      <td>0.497796+0.421212j</td>\n",
       "      <td>0.880716+0.957299j</td>\n",
       "      <td>-0.574380-0.344628j</td>\n",
       "      <td>-0.038292-0.038292j</td>\n",
       "      <td>-1.033883+0.114876j</td>\n",
       "      <td>-0.421212+1.187051j</td>\n",
       "      <td>-0.191460-0.114876j</td>\n",
       "      <td>-0.344628+0.191460j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>0.574380+0.268044j</td>\n",
       "      <td>0.650964-0.268044j</td>\n",
       "      <td>-1.110467-0.804132j</td>\n",
       "      <td>0.574380-0.574380j</td>\n",
       "      <td>-0.114876+0.268044j</td>\n",
       "      <td>0.804132+0.268044j</td>\n",
       "      <td>-0.804132+0.957299j</td>\n",
       "      <td>1.110467+1.110467j</td>\n",
       "      <td>-0.727548-0.804132j</td>\n",
       "      <td>-1.033883+0.191460j</td>\n",
       "      <td>-0.497796+0.344628j</td>\n",
       "      <td>-0.344628-0.421212j</td>\n",
       "      <td>-0.268044+0.114876j</td>\n",
       "      <td>-0.344628+0.574380j</td>\n",
       "      <td>0.880716+0.114876j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>-0.727548+0.421212j</td>\n",
       "      <td>0.344628+0.114876j</td>\n",
       "      <td>-0.421212+0.574380j</td>\n",
       "      <td>-0.650964-0.650964j</td>\n",
       "      <td>1.033883+0.268044j</td>\n",
       "      <td>1.110467-0.268044j</td>\n",
       "      <td>-1.033883-0.344628j</td>\n",
       "      <td>0.114876-0.650964j</td>\n",
       "      <td>0.038292+0.804132j</td>\n",
       "      <td>0.574380-0.191460j</td>\n",
       "      <td>0.650964-0.804132j</td>\n",
       "      <td>1.187051+0.114876j</td>\n",
       "      <td>0.727548-0.804132j</td>\n",
       "      <td>1.033883+0.344628j</td>\n",
       "      <td>0.114876+0.727548j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>-1.033883-0.497796j</td>\n",
       "      <td>0.421212-0.191460j</td>\n",
       "      <td>0.727548+0.574380j</td>\n",
       "      <td>-0.114876-0.114876j</td>\n",
       "      <td>0.038292+0.268044j</td>\n",
       "      <td>-1.187051+0.497796j</td>\n",
       "      <td>-0.344628-0.650964j</td>\n",
       "      <td>0.727548-0.114876j</td>\n",
       "      <td>-0.344628+0.804132j</td>\n",
       "      <td>-0.038292-0.344628j</td>\n",
       "      <td>-1.033883-0.421212j</td>\n",
       "      <td>0.114876-0.191460j</td>\n",
       "      <td>-0.038292-1.033883j</td>\n",
       "      <td>0.497796-0.191460j</td>\n",
       "      <td>-0.268044+0.344628j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>-1.033883+0.114876j</td>\n",
       "      <td>0.957299-0.804132j</td>\n",
       "      <td>-0.191460-0.574380j</td>\n",
       "      <td>0.804132-1.110467j</td>\n",
       "      <td>0.344628+0.038292j</td>\n",
       "      <td>-0.497796+0.421212j</td>\n",
       "      <td>-0.344628-0.727548j</td>\n",
       "      <td>-0.268044-0.038292j</td>\n",
       "      <td>-1.110467+0.727548j</td>\n",
       "      <td>0.421212+0.344628j</td>\n",
       "      <td>0.114876-0.957299j</td>\n",
       "      <td>1.187051-0.650964j</td>\n",
       "      <td>-1.033883+0.191460j</td>\n",
       "      <td>-1.110467+0.957299j</td>\n",
       "      <td>-0.957299-0.574380j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>-0.880716-1.033883j</td>\n",
       "      <td>1.033883+0.804132j</td>\n",
       "      <td>-1.187051-0.344628j</td>\n",
       "      <td>0.344628+1.110467j</td>\n",
       "      <td>-0.727548+0.114876j</td>\n",
       "      <td>1.110467+0.804132j</td>\n",
       "      <td>-1.110467+0.574380j</td>\n",
       "      <td>-0.038292-0.344628j</td>\n",
       "      <td>-0.038292-1.187051j</td>\n",
       "      <td>-0.880716+0.880716j</td>\n",
       "      <td>0.421212-1.110467j</td>\n",
       "      <td>-0.038292-0.957299j</td>\n",
       "      <td>1.187051-0.574380j</td>\n",
       "      <td>0.344628-0.727548j</td>\n",
       "      <td>0.114876+1.033883j</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0                   1                   2   \\\n",
       "0      0.574380+0.114876j  0.574380+0.038292j  0.191460+0.038292j   \n",
       "1     -0.880716+0.114876j  0.880716+1.187051j -0.650964+0.421212j   \n",
       "2      0.421212-0.114876j -0.114876-0.880716j -1.033883+0.957299j   \n",
       "3      0.574380+0.650964j  0.574380-0.804132j -0.804132-0.344628j   \n",
       "4     -0.880716-0.344628j  1.033883+0.804132j -0.650964-0.421212j   \n",
       "...                   ...                 ...                 ...   \n",
       "29995  0.574380+0.268044j  0.650964-0.268044j -1.110467-0.804132j   \n",
       "29996 -0.727548+0.421212j  0.344628+0.114876j -0.421212+0.574380j   \n",
       "29997 -1.033883-0.497796j  0.421212-0.191460j  0.727548+0.574380j   \n",
       "29998 -1.033883+0.114876j  0.957299-0.804132j -0.191460-0.574380j   \n",
       "29999 -0.880716-1.033883j  1.033883+0.804132j -1.187051-0.344628j   \n",
       "\n",
       "                       3                   4                   5   \\\n",
       "0     -0.421212+0.497796j  1.110467-0.344628j -0.344628-1.110467j   \n",
       "1     -0.268044+0.191460j  0.344628-0.268044j -0.114876+0.650964j   \n",
       "2      0.804132+0.497796j -0.114876+1.033883j  0.957299+0.191460j   \n",
       "3     -0.957299-0.957299j  0.421212+0.804132j -0.114876+0.650964j   \n",
       "4      0.268044-0.727548j -1.110467+0.114876j -1.187051-1.110467j   \n",
       "...                   ...                 ...                 ...   \n",
       "29995  0.574380-0.574380j -0.114876+0.268044j  0.804132+0.268044j   \n",
       "29996 -0.650964-0.650964j  1.033883+0.268044j  1.110467-0.268044j   \n",
       "29997 -0.114876-0.114876j  0.038292+0.268044j -1.187051+0.497796j   \n",
       "29998  0.804132-1.110467j  0.344628+0.038292j -0.497796+0.421212j   \n",
       "29999  0.344628+1.110467j -0.727548+0.114876j  1.110467+0.804132j   \n",
       "\n",
       "                       6                   7                   8   \\\n",
       "0     -0.804132-0.727548j -0.804132-0.038292j  0.344628-0.038292j   \n",
       "1     -1.110467-0.344628j  1.033883+0.421212j -0.497796-0.957299j   \n",
       "2      0.191460-1.110467j  0.191460-0.957299j  0.344628+0.727548j   \n",
       "3      0.038292+0.268044j  0.114876+0.268044j  0.497796-0.574380j   \n",
       "4      0.574380+0.421212j  0.497796+0.421212j  0.880716+0.957299j   \n",
       "...                   ...                 ...                 ...   \n",
       "29995 -0.804132+0.957299j  1.110467+1.110467j -0.727548-0.804132j   \n",
       "29996 -1.033883-0.344628j  0.114876-0.650964j  0.038292+0.804132j   \n",
       "29997 -0.344628-0.650964j  0.727548-0.114876j -0.344628+0.804132j   \n",
       "29998 -0.344628-0.727548j -0.268044-0.038292j -1.110467+0.727548j   \n",
       "29999 -1.110467+0.574380j -0.038292-0.344628j -0.038292-1.187051j   \n",
       "\n",
       "                       9                   10                  11  \\\n",
       "0      0.344628-0.114876j -0.038292+0.497796j  0.574380+0.957299j   \n",
       "1      0.650964+0.880716j -0.114876-0.114876j  0.574380-0.268044j   \n",
       "2      0.574380-0.650964j  0.497796+0.574380j -0.650964+0.421212j   \n",
       "3     -0.727548+1.110467j -0.650964+0.268044j -0.421212+0.344628j   \n",
       "4     -0.574380-0.344628j -0.038292-0.038292j -1.033883+0.114876j   \n",
       "...                   ...                 ...                 ...   \n",
       "29995 -1.033883+0.191460j -0.497796+0.344628j -0.344628-0.421212j   \n",
       "29996  0.574380-0.191460j  0.650964-0.804132j  1.187051+0.114876j   \n",
       "29997 -0.038292-0.344628j -1.033883-0.421212j  0.114876-0.191460j   \n",
       "29998  0.421212+0.344628j  0.114876-0.957299j  1.187051-0.650964j   \n",
       "29999 -0.880716+0.880716j  0.421212-1.110467j -0.038292-0.957299j   \n",
       "\n",
       "                       12                  13                  14  \n",
       "0     -1.187051+0.727548j  0.114876-0.344628j -0.038292+0.957299j  \n",
       "1      0.650964+0.880716j -0.727548+0.268044j  0.957299+0.497796j  \n",
       "2     -1.033883+0.574380j -0.268044+0.114876j -0.191460-1.110467j  \n",
       "3     -1.110467+0.804132j -0.114876+1.187051j -0.880716+1.187051j  \n",
       "4     -0.421212+1.187051j -0.191460-0.114876j -0.344628+0.191460j  \n",
       "...                   ...                 ...                 ...  \n",
       "29995 -0.268044+0.114876j -0.344628+0.574380j  0.880716+0.114876j  \n",
       "29996  0.727548-0.804132j  1.033883+0.344628j  0.114876+0.727548j  \n",
       "29997 -0.038292-1.033883j  0.497796-0.191460j -0.268044+0.344628j  \n",
       "29998 -1.033883+0.191460j -1.110467+0.957299j -0.957299-0.574380j  \n",
       "29999  1.187051-0.574380j  0.344628-0.727548j  0.114876+1.033883j  \n",
       "\n",
       "[30000 rows x 15 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target = df_target.drop(index=index_of_rows)\n",
    "df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature = df_feature[df_feature.loc[:,0] != 0j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>465</th>\n",
       "      <th>466</th>\n",
       "      <th>467</th>\n",
       "      <th>468</th>\n",
       "      <th>469</th>\n",
       "      <th>470</th>\n",
       "      <th>471</th>\n",
       "      <th>472</th>\n",
       "      <th>473</th>\n",
       "      <th>474</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.245799-0.292176j</td>\n",
       "      <td>-1.330345-0.318523j</td>\n",
       "      <td>-1.333179-0.327656j</td>\n",
       "      <td>-1.295741-0.327539j</td>\n",
       "      <td>-1.257863-0.325232j</td>\n",
       "      <td>-1.242229-0.324840j</td>\n",
       "      <td>-1.249740-0.327369j</td>\n",
       "      <td>-1.265334-0.330762j</td>\n",
       "      <td>-1.270650-0.331395j</td>\n",
       "      <td>-1.258896-0.326735j</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223757-0.379588j</td>\n",
       "      <td>0.244452-0.201714j</td>\n",
       "      <td>0.092920+0.175302j</td>\n",
       "      <td>-0.114768+0.635493j</td>\n",
       "      <td>-0.277130+1.017293j</td>\n",
       "      <td>-0.343242+1.189098j</td>\n",
       "      <td>-0.315785+1.108918j</td>\n",
       "      <td>-0.231035+0.835668j</td>\n",
       "      <td>-0.132680+0.489787j</td>\n",
       "      <td>-0.052745+0.190412j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.106911+0.491718j</td>\n",
       "      <td>1.177100+0.521656j</td>\n",
       "      <td>1.172034+0.520385j</td>\n",
       "      <td>1.131914+0.507528j</td>\n",
       "      <td>1.095133+0.499688j</td>\n",
       "      <td>1.082941+0.503957j</td>\n",
       "      <td>1.095336+0.517099j</td>\n",
       "      <td>1.116791+0.529557j</td>\n",
       "      <td>1.128315+0.531811j</td>\n",
       "      <td>1.121248+0.520812j</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951374+0.052800j</td>\n",
       "      <td>0.759483-0.036866j</td>\n",
       "      <td>0.339731-0.312025j</td>\n",
       "      <td>-0.170124-0.652597j</td>\n",
       "      <td>-0.597819-0.921273j</td>\n",
       "      <td>-0.816160-1.019368j</td>\n",
       "      <td>-0.794928-0.922378j</td>\n",
       "      <td>-0.599373-0.680918j</td>\n",
       "      <td>-0.342885-0.389879j</td>\n",
       "      <td>-0.126636-0.142680j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.018773+1.244856j</td>\n",
       "      <td>0.021368+1.319193j</td>\n",
       "      <td>0.024165+1.313594j</td>\n",
       "      <td>0.023490+1.275121j</td>\n",
       "      <td>0.018128+1.244869j</td>\n",
       "      <td>0.010646+1.241904j</td>\n",
       "      <td>0.005653+1.261579j</td>\n",
       "      <td>0.006373+1.284680j</td>\n",
       "      <td>0.012243+1.291802j</td>\n",
       "      <td>0.018959+1.277444j</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140435+0.337597j</td>\n",
       "      <td>-0.133219+0.260295j</td>\n",
       "      <td>-0.506815+0.242203j</td>\n",
       "      <td>-0.917298+0.242280j</td>\n",
       "      <td>-1.249729+0.230142j</td>\n",
       "      <td>-1.389536+0.196261j</td>\n",
       "      <td>-1.283955+0.146636j</td>\n",
       "      <td>-0.973925+0.091823j</td>\n",
       "      <td>-0.575025+0.040488j</td>\n",
       "      <td>-0.219282-0.000987j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.295474-0.280640j</td>\n",
       "      <td>1.375535-0.299172j</td>\n",
       "      <td>1.371563-0.299915j</td>\n",
       "      <td>1.331481-0.291760j</td>\n",
       "      <td>1.298031-0.283309j</td>\n",
       "      <td>1.291666-0.280071j</td>\n",
       "      <td>1.308322-0.283263j</td>\n",
       "      <td>1.329003-0.290310j</td>\n",
       "      <td>1.335002-0.297024j</td>\n",
       "      <td>1.322205-0.300643j</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.129886-1.593995j</td>\n",
       "      <td>-0.059573-1.652626j</td>\n",
       "      <td>0.190969-1.739536j</td>\n",
       "      <td>0.509871-1.828353j</td>\n",
       "      <td>0.767413-1.860173j</td>\n",
       "      <td>0.870409-1.767205j</td>\n",
       "      <td>0.795386-1.512713j</td>\n",
       "      <td>0.587682-1.121238j</td>\n",
       "      <td>0.331428-0.676545j</td>\n",
       "      <td>0.108752-0.284741j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.802838-0.766358j</td>\n",
       "      <td>-0.852347-0.813439j</td>\n",
       "      <td>-0.847930-0.812228j</td>\n",
       "      <td>-0.820891-0.790825j</td>\n",
       "      <td>-0.799941-0.773245j</td>\n",
       "      <td>-0.799005-0.770460j</td>\n",
       "      <td>-0.814688-0.779914j</td>\n",
       "      <td>-0.832167-0.790823j</td>\n",
       "      <td>-0.836501-0.792250j</td>\n",
       "      <td>-0.823647-0.781384j</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087930-0.242369j</td>\n",
       "      <td>-0.068578-0.280977j</td>\n",
       "      <td>-0.157946-0.247598j</td>\n",
       "      <td>-0.289956-0.190257j</td>\n",
       "      <td>-0.395097-0.141364j</td>\n",
       "      <td>-0.429473-0.108417j</td>\n",
       "      <td>-0.386520-0.082776j</td>\n",
       "      <td>-0.290129-0.055175j</td>\n",
       "      <td>-0.177237-0.025308j</td>\n",
       "      <td>-0.080350-0.000589j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>-0.591411-0.946587j</td>\n",
       "      <td>-0.631333-1.003882j</td>\n",
       "      <td>-0.632693-0.998537j</td>\n",
       "      <td>-0.615195-0.966185j</td>\n",
       "      <td>-0.596590-0.939667j</td>\n",
       "      <td>-0.586312-0.935106j</td>\n",
       "      <td>-0.584751-0.948876j</td>\n",
       "      <td>-0.586570-0.964652j</td>\n",
       "      <td>-0.585496-0.966410j</td>\n",
       "      <td>-0.579427-0.950975j</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.745403-0.004432j</td>\n",
       "      <td>-0.603040+0.153681j</td>\n",
       "      <td>-0.333364+0.382821j</td>\n",
       "      <td>-0.013515+0.632996j</td>\n",
       "      <td>0.257330+0.827564j</td>\n",
       "      <td>0.404964+0.897226j</td>\n",
       "      <td>0.411455+0.815245j</td>\n",
       "      <td>0.314687+0.612213j</td>\n",
       "      <td>0.180684+0.361055j</td>\n",
       "      <td>0.067738+0.140979j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>1.414226-0.558196j</td>\n",
       "      <td>1.500999-0.596972j</td>\n",
       "      <td>1.492131-0.602040j</td>\n",
       "      <td>1.437303-0.592868j</td>\n",
       "      <td>1.383923-0.585465j</td>\n",
       "      <td>1.359225-0.585929j</td>\n",
       "      <td>1.364790-0.590859j</td>\n",
       "      <td>1.383077-0.592651j</td>\n",
       "      <td>1.391935-0.585949j</td>\n",
       "      <td>1.381941-0.572388j</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.675473+0.076402j</td>\n",
       "      <td>-1.568001-0.163546j</td>\n",
       "      <td>-1.316565-0.408754j</td>\n",
       "      <td>-1.014707-0.661175j</td>\n",
       "      <td>-0.744910-0.870518j</td>\n",
       "      <td>-0.544256-0.962972j</td>\n",
       "      <td>-0.402459-0.892619j</td>\n",
       "      <td>-0.287181-0.677981j</td>\n",
       "      <td>-0.175038-0.397339j</td>\n",
       "      <td>-0.067040-0.147374j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>0.722211-1.194098j</td>\n",
       "      <td>0.766435-1.271573j</td>\n",
       "      <td>0.760854-1.273961j</td>\n",
       "      <td>0.735113-1.245743j</td>\n",
       "      <td>0.714173-1.224247j</td>\n",
       "      <td>0.708246-1.225443j</td>\n",
       "      <td>0.713131-1.243518j</td>\n",
       "      <td>0.717808-1.260203j</td>\n",
       "      <td>0.714080-1.258104j</td>\n",
       "      <td>0.703292-1.233477j</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105733+0.736649j</td>\n",
       "      <td>-0.217215+0.551022j</td>\n",
       "      <td>-0.246886+0.251756j</td>\n",
       "      <td>-0.244449-0.097723j</td>\n",
       "      <td>-0.239509-0.398960j</td>\n",
       "      <td>-0.232995-0.565116j</td>\n",
       "      <td>-0.210805-0.563894j</td>\n",
       "      <td>-0.164268-0.430281j</td>\n",
       "      <td>-0.101104-0.242452j</td>\n",
       "      <td>-0.040537-0.078296j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>0.749197+1.069434j</td>\n",
       "      <td>0.793965+1.148567j</td>\n",
       "      <td>0.792032+1.157250j</td>\n",
       "      <td>0.771860+1.130366j</td>\n",
       "      <td>0.757186+1.101022j</td>\n",
       "      <td>0.757277+1.087773j</td>\n",
       "      <td>0.766513+1.091279j</td>\n",
       "      <td>0.771764+1.099125j</td>\n",
       "      <td>0.762402+1.096343j</td>\n",
       "      <td>0.738889+1.077893j</td>\n",
       "      <td>...</td>\n",
       "      <td>1.846746+0.453473j</td>\n",
       "      <td>1.664992+0.588680j</td>\n",
       "      <td>1.291616+0.852089j</td>\n",
       "      <td>0.824895+1.155062j</td>\n",
       "      <td>0.401593+1.383355j</td>\n",
       "      <td>0.122026+1.440803j</td>\n",
       "      <td>0.004537+1.290577j</td>\n",
       "      <td>-0.004731+0.972788j</td>\n",
       "      <td>0.017895+0.586556j</td>\n",
       "      <td>0.022631+0.244009j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>0.954786+0.770151j</td>\n",
       "      <td>1.012539+0.811824j</td>\n",
       "      <td>1.005158+0.807281j</td>\n",
       "      <td>0.968209+0.784296j</td>\n",
       "      <td>0.935026+0.765948j</td>\n",
       "      <td>0.923357+0.762145j</td>\n",
       "      <td>0.932256+0.769557j</td>\n",
       "      <td>0.947737+0.777538j</td>\n",
       "      <td>0.953757+0.776451j</td>\n",
       "      <td>0.944128+0.765273j</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.862934+0.455498j</td>\n",
       "      <td>-0.595929+0.258505j</td>\n",
       "      <td>-0.254154-0.118325j</td>\n",
       "      <td>0.120527-0.555753j</td>\n",
       "      <td>0.441699-0.907743j</td>\n",
       "      <td>0.621632-1.062517j</td>\n",
       "      <td>0.621813-0.988103j</td>\n",
       "      <td>0.475748-0.740069j</td>\n",
       "      <td>0.269149-0.428069j</td>\n",
       "      <td>0.092172-0.160869j</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 475 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0                   1                   2    \\\n",
       "0     -1.245799-0.292176j -1.330345-0.318523j -1.333179-0.327656j   \n",
       "1      1.106911+0.491718j  1.177100+0.521656j  1.172034+0.520385j   \n",
       "2      0.018773+1.244856j  0.021368+1.319193j  0.024165+1.313594j   \n",
       "3      1.295474-0.280640j  1.375535-0.299172j  1.371563-0.299915j   \n",
       "4     -0.802838-0.766358j -0.852347-0.813439j -0.847930-0.812228j   \n",
       "...                   ...                 ...                 ...   \n",
       "29995 -0.591411-0.946587j -0.631333-1.003882j -0.632693-0.998537j   \n",
       "29996  1.414226-0.558196j  1.500999-0.596972j  1.492131-0.602040j   \n",
       "29997  0.722211-1.194098j  0.766435-1.271573j  0.760854-1.273961j   \n",
       "29998  0.749197+1.069434j  0.793965+1.148567j  0.792032+1.157250j   \n",
       "29999  0.954786+0.770151j  1.012539+0.811824j  1.005158+0.807281j   \n",
       "\n",
       "                      3                   4                   5    \\\n",
       "0     -1.295741-0.327539j -1.257863-0.325232j -1.242229-0.324840j   \n",
       "1      1.131914+0.507528j  1.095133+0.499688j  1.082941+0.503957j   \n",
       "2      0.023490+1.275121j  0.018128+1.244869j  0.010646+1.241904j   \n",
       "3      1.331481-0.291760j  1.298031-0.283309j  1.291666-0.280071j   \n",
       "4     -0.820891-0.790825j -0.799941-0.773245j -0.799005-0.770460j   \n",
       "...                   ...                 ...                 ...   \n",
       "29995 -0.615195-0.966185j -0.596590-0.939667j -0.586312-0.935106j   \n",
       "29996  1.437303-0.592868j  1.383923-0.585465j  1.359225-0.585929j   \n",
       "29997  0.735113-1.245743j  0.714173-1.224247j  0.708246-1.225443j   \n",
       "29998  0.771860+1.130366j  0.757186+1.101022j  0.757277+1.087773j   \n",
       "29999  0.968209+0.784296j  0.935026+0.765948j  0.923357+0.762145j   \n",
       "\n",
       "                      6                   7                   8    \\\n",
       "0     -1.249740-0.327369j -1.265334-0.330762j -1.270650-0.331395j   \n",
       "1      1.095336+0.517099j  1.116791+0.529557j  1.128315+0.531811j   \n",
       "2      0.005653+1.261579j  0.006373+1.284680j  0.012243+1.291802j   \n",
       "3      1.308322-0.283263j  1.329003-0.290310j  1.335002-0.297024j   \n",
       "4     -0.814688-0.779914j -0.832167-0.790823j -0.836501-0.792250j   \n",
       "...                   ...                 ...                 ...   \n",
       "29995 -0.584751-0.948876j -0.586570-0.964652j -0.585496-0.966410j   \n",
       "29996  1.364790-0.590859j  1.383077-0.592651j  1.391935-0.585949j   \n",
       "29997  0.713131-1.243518j  0.717808-1.260203j  0.714080-1.258104j   \n",
       "29998  0.766513+1.091279j  0.771764+1.099125j  0.762402+1.096343j   \n",
       "29999  0.932256+0.769557j  0.947737+0.777538j  0.953757+0.776451j   \n",
       "\n",
       "                      9    ...                 465                 466  \\\n",
       "0     -1.258896-0.326735j  ...  0.223757-0.379588j  0.244452-0.201714j   \n",
       "1      1.121248+0.520812j  ...  0.951374+0.052800j  0.759483-0.036866j   \n",
       "2      0.018959+1.277444j  ...  0.140435+0.337597j -0.133219+0.260295j   \n",
       "3      1.322205-0.300643j  ... -0.129886-1.593995j -0.059573-1.652626j   \n",
       "4     -0.823647-0.781384j  ... -0.087930-0.242369j -0.068578-0.280977j   \n",
       "...                   ...  ...                 ...                 ...   \n",
       "29995 -0.579427-0.950975j  ... -0.745403-0.004432j -0.603040+0.153681j   \n",
       "29996  1.381941-0.572388j  ... -1.675473+0.076402j -1.568001-0.163546j   \n",
       "29997  0.703292-1.233477j  ... -0.105733+0.736649j -0.217215+0.551022j   \n",
       "29998  0.738889+1.077893j  ...  1.846746+0.453473j  1.664992+0.588680j   \n",
       "29999  0.944128+0.765273j  ... -0.862934+0.455498j -0.595929+0.258505j   \n",
       "\n",
       "                      467                 468                 469  \\\n",
       "0      0.092920+0.175302j -0.114768+0.635493j -0.277130+1.017293j   \n",
       "1      0.339731-0.312025j -0.170124-0.652597j -0.597819-0.921273j   \n",
       "2     -0.506815+0.242203j -0.917298+0.242280j -1.249729+0.230142j   \n",
       "3      0.190969-1.739536j  0.509871-1.828353j  0.767413-1.860173j   \n",
       "4     -0.157946-0.247598j -0.289956-0.190257j -0.395097-0.141364j   \n",
       "...                   ...                 ...                 ...   \n",
       "29995 -0.333364+0.382821j -0.013515+0.632996j  0.257330+0.827564j   \n",
       "29996 -1.316565-0.408754j -1.014707-0.661175j -0.744910-0.870518j   \n",
       "29997 -0.246886+0.251756j -0.244449-0.097723j -0.239509-0.398960j   \n",
       "29998  1.291616+0.852089j  0.824895+1.155062j  0.401593+1.383355j   \n",
       "29999 -0.254154-0.118325j  0.120527-0.555753j  0.441699-0.907743j   \n",
       "\n",
       "                      470                 471                 472  \\\n",
       "0     -0.343242+1.189098j -0.315785+1.108918j -0.231035+0.835668j   \n",
       "1     -0.816160-1.019368j -0.794928-0.922378j -0.599373-0.680918j   \n",
       "2     -1.389536+0.196261j -1.283955+0.146636j -0.973925+0.091823j   \n",
       "3      0.870409-1.767205j  0.795386-1.512713j  0.587682-1.121238j   \n",
       "4     -0.429473-0.108417j -0.386520-0.082776j -0.290129-0.055175j   \n",
       "...                   ...                 ...                 ...   \n",
       "29995  0.404964+0.897226j  0.411455+0.815245j  0.314687+0.612213j   \n",
       "29996 -0.544256-0.962972j -0.402459-0.892619j -0.287181-0.677981j   \n",
       "29997 -0.232995-0.565116j -0.210805-0.563894j -0.164268-0.430281j   \n",
       "29998  0.122026+1.440803j  0.004537+1.290577j -0.004731+0.972788j   \n",
       "29999  0.621632-1.062517j  0.621813-0.988103j  0.475748-0.740069j   \n",
       "\n",
       "                      473                 474  \n",
       "0     -0.132680+0.489787j -0.052745+0.190412j  \n",
       "1     -0.342885-0.389879j -0.126636-0.142680j  \n",
       "2     -0.575025+0.040488j -0.219282-0.000987j  \n",
       "3      0.331428-0.676545j  0.108752-0.284741j  \n",
       "4     -0.177237-0.025308j -0.080350-0.000589j  \n",
       "...                   ...                 ...  \n",
       "29995  0.180684+0.361055j  0.067738+0.140979j  \n",
       "29996 -0.175038-0.397339j -0.067040-0.147374j  \n",
       "29997 -0.101104-0.242452j -0.040537-0.078296j  \n",
       "29998  0.017895+0.586556j  0.022631+0.244009j  \n",
       "29999  0.269149-0.428069j  0.092172-0.160869j  \n",
       "\n",
       "[30000 rows x 475 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_symbol = df_target.to_numpy()\n",
    "\n",
    "real_target_symbol = np.real(target_symbol)\n",
    "imag_target_symbol = np.imag(target_symbol)\n",
    "\n",
    "y = np.array([real_target_symbol, imag_target_symbol]).transpose(1, 2, 0)\n",
    "\n",
    "\n",
    "feature_symbol = df_feature.to_numpy()\n",
    "real_feature_symbol = np.real(feature_symbol)\n",
    "imag_feature_symbol = np.imag(feature_symbol)\n",
    "\n",
    "X = np.array([real_feature_symbol, imag_feature_symbol]).transpose(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.57437969,  0.11487594],\n",
       "        [ 0.57437969,  0.03829198],\n",
       "        [ 0.1914599 ,  0.03829198],\n",
       "        ...,\n",
       "        [-1.18705135,  0.7275476 ],\n",
       "        [ 0.11487594, -0.34462781],\n",
       "        [-0.03829198,  0.95729948]],\n",
       "\n",
       "       [[-0.88071552,  0.11487594],\n",
       "        [ 0.88071552,  1.18705135],\n",
       "        [-0.65096364,  0.42121177],\n",
       "        ...,\n",
       "        [ 0.65096364,  0.88071552],\n",
       "        [-0.7275476 ,  0.26804385],\n",
       "        [ 0.95729948,  0.49779573]],\n",
       "\n",
       "       [[ 0.42121177, -0.11487594],\n",
       "        [-0.11487594, -0.88071552],\n",
       "        [-1.03388343,  0.95729948],\n",
       "        ...,\n",
       "        [-1.03388343,  0.57437969],\n",
       "        [-0.26804385,  0.11487594],\n",
       "        [-0.1914599 , -1.11046739]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.03388343, -0.49779573],\n",
       "        [ 0.42121177, -0.1914599 ],\n",
       "        [ 0.7275476 ,  0.57437969],\n",
       "        ...,\n",
       "        [-0.03829198, -1.03388343],\n",
       "        [ 0.49779573, -0.1914599 ],\n",
       "        [-0.26804385,  0.34462781]],\n",
       "\n",
       "       [[-1.03388343,  0.11487594],\n",
       "        [ 0.95729948, -0.80413156],\n",
       "        [-0.1914599 , -0.57437969],\n",
       "        ...,\n",
       "        [-1.03388343,  0.1914599 ],\n",
       "        [-1.11046739,  0.95729948],\n",
       "        [-0.95729948, -0.57437969]],\n",
       "\n",
       "       [[-0.88071552, -1.03388343],\n",
       "        [ 1.03388343,  0.80413156],\n",
       "        [-1.18705135, -0.34462781],\n",
       "        ...,\n",
       "        [ 1.18705135, -0.57437969],\n",
       "        [ 0.34462781, -0.7275476 ],\n",
       "        [ 0.11487594,  1.03388343]]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.24579929e+00, -2.92176498e-01],\n",
       "        [-1.33034466e+00, -3.18523467e-01],\n",
       "        [-1.33317900e+00, -3.27656059e-01],\n",
       "        ...,\n",
       "        [-2.31035171e-01,  8.35667791e-01],\n",
       "        [-1.32679638e-01,  4.89787191e-01],\n",
       "        [-5.27448015e-02,  1.90412222e-01]],\n",
       "\n",
       "       [[ 1.10691094e+00,  4.91718224e-01],\n",
       "        [ 1.17710002e+00,  5.21655622e-01],\n",
       "        [ 1.17203411e+00,  5.20384510e-01],\n",
       "        ...,\n",
       "        [-5.99372615e-01, -6.80917566e-01],\n",
       "        [-3.42885332e-01, -3.89878988e-01],\n",
       "        [-1.26636151e-01, -1.42679808e-01]],\n",
       "\n",
       "       [[ 1.87729445e-02,  1.24485557e+00],\n",
       "        [ 2.13683243e-02,  1.31919320e+00],\n",
       "        [ 2.41648558e-02,  1.31359442e+00],\n",
       "        ...,\n",
       "        [-9.73924909e-01,  9.18232059e-02],\n",
       "        [-5.75024614e-01,  4.04881646e-02],\n",
       "        [-2.19282050e-01, -9.86593105e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 7.22211107e-01, -1.19409807e+00],\n",
       "        [ 7.66435403e-01, -1.27157321e+00],\n",
       "        [ 7.60853510e-01, -1.27396071e+00],\n",
       "        ...,\n",
       "        [-1.64268353e-01, -4.30280756e-01],\n",
       "        [-1.01103719e-01, -2.42452348e-01],\n",
       "        [-4.05374110e-02, -7.82959606e-02]],\n",
       "\n",
       "       [[ 7.49197131e-01,  1.06943437e+00],\n",
       "        [ 7.93965352e-01,  1.14856736e+00],\n",
       "        [ 7.92031982e-01,  1.15724960e+00],\n",
       "        ...,\n",
       "        [-4.73077305e-03,  9.72788230e-01],\n",
       "        [ 1.78951840e-02,  5.86556213e-01],\n",
       "        [ 2.26306346e-02,  2.44009267e-01]],\n",
       "\n",
       "       [[ 9.54785555e-01,  7.70150712e-01],\n",
       "        [ 1.01253887e+00,  8.11823517e-01],\n",
       "        [ 1.00515832e+00,  8.07281332e-01],\n",
       "        ...,\n",
       "        [ 4.75747740e-01, -7.40069295e-01],\n",
       "        [ 2.69149178e-01, -4.28069382e-01],\n",
       "        [ 9.21722632e-02, -1.60868908e-01]]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 475, 2)\n",
      "(30000, 15, 2)\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 2)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymbolDataset(Dataset):\n",
    "    \"\"\"Class to create the torch Dataset object\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        super(SymbolDataset, self).__init__()\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.X[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Total number of samples\"\"\"\n",
    "        return len(self.X[:,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lenght of sequence given to encoder\n",
    "gt = 475\n",
    "# length of sequence given to decoder\n",
    "horizon = 15\n",
    "\n",
    "# defining batch size\n",
    "batch_size = 64\n",
    "\n",
    "# creating torch dataset and dataloaders\n",
    "symbol_dataset = SymbolDataset(X,y)\n",
    "train_loader = DataLoader(symbol_dataset, batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining model save location\n",
    "save_location = \"./Transformer_models\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tf_model = model_Transformer_modified.Transformer(encoder_input_size=2, decoder_input_size=2,\n",
    "                                embedding_size=32, num_heads=4, num_layers=6, feedforward_size=1024).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, loss_fn, train_loader, device, print_every):\n",
    "    # Train:\n",
    "    model.train()\n",
    "    train_loss_batches = []\n",
    "\n",
    "    num_batches = len(train_loader)\n",
    "    \n",
    "    for batch_index, (x, y) in enumerate(train_loader, 1):\n",
    "        \n",
    "        feature, target = x.float().to(device), y.float().to(device)\n",
    "\n",
    "        # initializing a sequence for decoding by concatenating a start-of-sequence tensor with the target tensor along the second dimension.\n",
    "        start_of_seq = torch.Tensor([0,1]).unsqueeze(0).unsqueeze(1).repeat(target.shape[0],1, 1).to(device)\n",
    "        dec_input = torch.cat((start_of_seq, target[:,:-1,:]), 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model.forward(feature, dec_input)\n",
    "        \n",
    "        loss = loss_fn(predictions.view(feature.size(0), -1), target.contiguous().view(feature.size(0), -1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_batches.append(loss.item())\n",
    "\n",
    "        # If you want to print your progress more often than every epoch you can\n",
    "        # set `print_every` to the number of batches you want between every status update.\n",
    "        # Note that the print out will trigger a full validation on the full val. set => slows down training\n",
    "        if print_every is not None and batch_index % print_every == 0:\n",
    "            model.train()\n",
    "            print(f\"\\tBatch {batch_index}/{num_batches}: \"\n",
    "                  f\"\\tTrain loss: {sum(train_loss_batches[-print_every:])/print_every:.3f}\")\n",
    "\n",
    "    return model, train_loss_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, optimizer, loss_fn, train_loader, num_epochs, print_every):\n",
    "    print(\"Starting training\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model, train_loss = train_epoch(model, optimizer, loss_fn, train_loader, device, print_every)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{num_epochs}: \"\n",
    "              f\"Train loss: {sum(train_loss)/len(train_loss):.3f}\")\n",
    "        \n",
    "        train_losses.extend(train_loss)\n",
    "\n",
    "        if (epoch)%10 == 0:\n",
    "            # Saving model, loss and error log files\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'training_loss': train_loss\n",
    "                }, os.path.join(save_location, 'Transformer_based_channel_model_fsfd3_epoch{}.pth'.format(epoch)))\n",
    "        \n",
    "        # if (epoch)%10 == 0:\n",
    "        # # Saving model, loss and error log files\n",
    "        #     torch.save(model, os.path.join(save_location, 'Transformer_based_channel_model_epoch{}.pt'.format(epoch)))\n",
    "\n",
    "    return model, train_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "\tBatch 100/469: \tTrain loss: 0.101\n",
      "\tBatch 200/469: \tTrain loss: 0.068\n",
      "\tBatch 300/469: \tTrain loss: 0.065\n",
      "\tBatch 400/469: \tTrain loss: 0.063\n",
      "Epoch 1/100: Train loss: 0.072\n",
      "\tBatch 100/469: \tTrain loss: 0.061\n",
      "\tBatch 200/469: \tTrain loss: 0.058\n",
      "\tBatch 300/469: \tTrain loss: 0.055\n",
      "\tBatch 400/469: \tTrain loss: 0.050\n",
      "Epoch 2/100: Train loss: 0.054\n",
      "\tBatch 100/469: \tTrain loss: 0.038\n",
      "\tBatch 200/469: \tTrain loss: 0.031\n",
      "\tBatch 300/469: \tTrain loss: 0.026\n",
      "\tBatch 400/469: \tTrain loss: 0.021\n",
      "Epoch 3/100: Train loss: 0.027\n",
      "\tBatch 100/469: \tTrain loss: 0.015\n",
      "\tBatch 200/469: \tTrain loss: 0.014\n",
      "\tBatch 300/469: \tTrain loss: 0.013\n",
      "\tBatch 400/469: \tTrain loss: 0.012\n",
      "Epoch 4/100: Train loss: 0.013\n",
      "\tBatch 100/469: \tTrain loss: 0.011\n",
      "\tBatch 200/469: \tTrain loss: 0.011\n",
      "\tBatch 300/469: \tTrain loss: 0.011\n",
      "\tBatch 400/469: \tTrain loss: 0.011\n",
      "Epoch 5/100: Train loss: 0.011\n",
      "\tBatch 100/469: \tTrain loss: 0.010\n",
      "\tBatch 200/469: \tTrain loss: 0.010\n",
      "\tBatch 300/469: \tTrain loss: 0.010\n",
      "\tBatch 400/469: \tTrain loss: 0.010\n",
      "Epoch 6/100: Train loss: 0.010\n",
      "\tBatch 100/469: \tTrain loss: 0.009\n",
      "\tBatch 200/469: \tTrain loss: 0.010\n",
      "\tBatch 300/469: \tTrain loss: 0.009\n",
      "\tBatch 400/469: \tTrain loss: 0.009\n",
      "Epoch 7/100: Train loss: 0.009\n",
      "\tBatch 100/469: \tTrain loss: 0.009\n",
      "\tBatch 200/469: \tTrain loss: 0.009\n",
      "\tBatch 300/469: \tTrain loss: 0.009\n",
      "\tBatch 400/469: \tTrain loss: 0.009\n",
      "Epoch 8/100: Train loss: 0.009\n",
      "\tBatch 100/469: \tTrain loss: 0.009\n",
      "\tBatch 200/469: \tTrain loss: 0.008\n",
      "\tBatch 300/469: \tTrain loss: 0.008\n",
      "\tBatch 400/469: \tTrain loss: 0.008\n",
      "Epoch 9/100: Train loss: 0.008\n",
      "\tBatch 100/469: \tTrain loss: 0.008\n",
      "\tBatch 200/469: \tTrain loss: 0.008\n",
      "\tBatch 300/469: \tTrain loss: 0.008\n",
      "\tBatch 400/469: \tTrain loss: 0.008\n",
      "Epoch 10/100: Train loss: 0.008\n",
      "\tBatch 100/469: \tTrain loss: 0.008\n",
      "\tBatch 200/469: \tTrain loss: 0.008\n",
      "\tBatch 300/469: \tTrain loss: 0.007\n",
      "\tBatch 400/469: \tTrain loss: 0.007\n",
      "Epoch 11/100: Train loss: 0.007\n",
      "\tBatch 100/469: \tTrain loss: 0.007\n",
      "\tBatch 200/469: \tTrain loss: 0.007\n",
      "\tBatch 300/469: \tTrain loss: 0.007\n",
      "\tBatch 400/469: \tTrain loss: 0.006\n",
      "Epoch 12/100: Train loss: 0.007\n",
      "\tBatch 100/469: \tTrain loss: 0.006\n",
      "\tBatch 200/469: \tTrain loss: 0.005\n",
      "\tBatch 300/469: \tTrain loss: 0.005\n",
      "\tBatch 400/469: \tTrain loss: 0.004\n",
      "Epoch 13/100: Train loss: 0.005\n",
      "\tBatch 100/469: \tTrain loss: 0.004\n",
      "\tBatch 200/469: \tTrain loss: 0.003\n",
      "\tBatch 300/469: \tTrain loss: 0.003\n",
      "\tBatch 400/469: \tTrain loss: 0.003\n",
      "Epoch 14/100: Train loss: 0.003\n",
      "\tBatch 100/469: \tTrain loss: 0.003\n",
      "\tBatch 200/469: \tTrain loss: 0.003\n",
      "\tBatch 300/469: \tTrain loss: 0.003\n",
      "\tBatch 400/469: \tTrain loss: 0.002\n",
      "Epoch 15/100: Train loss: 0.003\n",
      "\tBatch 100/469: \tTrain loss: 0.002\n",
      "\tBatch 200/469: \tTrain loss: 0.002\n",
      "\tBatch 300/469: \tTrain loss: 0.002\n",
      "\tBatch 400/469: \tTrain loss: 0.002\n",
      "Epoch 16/100: Train loss: 0.002\n",
      "\tBatch 100/469: \tTrain loss: 0.002\n",
      "\tBatch 200/469: \tTrain loss: 0.002\n",
      "\tBatch 300/469: \tTrain loss: 0.002\n",
      "\tBatch 400/469: \tTrain loss: 0.002\n",
      "Epoch 17/100: Train loss: 0.002\n",
      "\tBatch 100/469: \tTrain loss: 0.002\n",
      "\tBatch 200/469: \tTrain loss: 0.002\n",
      "\tBatch 300/469: \tTrain loss: 0.002\n",
      "\tBatch 400/469: \tTrain loss: 0.002\n",
      "Epoch 18/100: Train loss: 0.002\n",
      "\tBatch 100/469: \tTrain loss: 0.002\n",
      "\tBatch 200/469: \tTrain loss: 0.002\n",
      "\tBatch 300/469: \tTrain loss: 0.002\n",
      "\tBatch 400/469: \tTrain loss: 0.002\n",
      "Epoch 19/100: Train loss: 0.002\n",
      "\tBatch 100/469: \tTrain loss: 0.002\n",
      "\tBatch 200/469: \tTrain loss: 0.001\n",
      "\tBatch 300/469: \tTrain loss: 0.001\n",
      "\tBatch 400/469: \tTrain loss: 0.001\n",
      "Epoch 20/100: Train loss: 0.001\n",
      "\tBatch 100/469: \tTrain loss: 0.001\n",
      "\tBatch 200/469: \tTrain loss: 0.001\n",
      "\tBatch 300/469: \tTrain loss: 0.001\n",
      "\tBatch 400/469: \tTrain loss: 0.001\n",
      "Epoch 21/100: Train loss: 0.001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[186], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# optimizer = torch.optim.AdamW(tf_model.parameters(), lr=1e-2, betas=(0.9, 0.95), weight_decay=1e-1)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# number of epochs \u001b[39;00m\n\u001b[1;32m     10\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m---> 12\u001b[0m Trained_model, train_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[185], line 8\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(model, optimizer, loss_fn, train_loader, num_epochs, print_every)\u001b[0m\n\u001b[1;32m      5\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     model, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(train_loss)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loss)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mextend(train_loss)\n",
      "Cell \u001b[0;32mIn[184], line 22\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, loss_fn, train_loader, device, print_every)\u001b[0m\n\u001b[1;32m     18\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(feature, dec_input)\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(predictions\u001b[38;5;241m.\u001b[39mview(feature\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), target\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(feature\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 22\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m train_loss_batches\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/conda/envs/dml/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/dml/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loss function\n",
    "loss_fn = nn.MSELoss().float()\n",
    "\n",
    "# creating optimizer\n",
    "# optimizer = torch.optim.SGD(tf_model.parameters(), lr=1e-4, momentum=0.9, weight_decay=1e-3, nesterov=True)\n",
    "optimizer = torch.optim.Adam(tf_model.parameters(), lr=1e-4)\n",
    "# optimizer = torch.optim.AdamW(tf_model.parameters(), lr=1e-2, betas=(0.9, 0.95), weight_decay=1e-1)\n",
    "\n",
    "# number of epochs \n",
    "num_epochs = 100\n",
    "\n",
    "Trained_model, train_losses = training_loop(tf_model, optimizer, loss_fn, train_loader, num_epochs, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
